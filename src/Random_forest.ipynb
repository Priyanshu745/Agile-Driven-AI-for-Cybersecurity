{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwVq/4JS06BJOdTtfXEgXy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"3aKAq5_YVz9v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740723268608,"user_tz":-330,"elapsed":65325,"user":{"displayName":"PRIYANSHU","userId":"14777312583550931565"}},"outputId":"39da3add-014e-473b-92f0-843243b5f1c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","==== Training Random Forest Models for Each Dataset ====\n","\n","\n","🔹 Training on NSLKDD dataset...\n","\n","✅ NSLKDD dataset limited to 100000 rows.\n","Class distribution in training set: {0: 35114, 1: 34886}\n","🎯 Random Forest Accuracy for NSLKDD: 0.9957\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     14887\n","           1       1.00      1.00      1.00     15113\n","\n","    accuracy                           1.00     30000\n","   macro avg       1.00      1.00      1.00     30000\n","weighted avg       1.00      1.00      1.00     30000\n","\n","\n","🔹 Training on UNSW_NB15 dataset...\n","\n","✅ UNSW_NB15 dataset limited to 100000 rows.\n","Class distribution in training set: {0: 25278, 1: 44722}\n","🎯 Random Forest Accuracy for UNSW_NB15: 1.0000\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     10805\n","           1       1.00      1.00      1.00     19195\n","\n","    accuracy                           1.00     30000\n","   macro avg       1.00      1.00      1.00     30000\n","weighted avg       1.00      1.00      1.00     30000\n","\n","\n","🔹 Training on KDDCup dataset...\n","\n","✅ KDDCup dataset limited to 100000 rows.\n","Class distribution in training set: {0: 32, 1: 1, 3: 176, 4: 2, 5: 2, 6: 8724, 7: 48, 8: 14918, 10: 134, 11: 360, 12: 45577, 13: 8, 14: 18}\n","🎯 Random Forest Accuracy for KDDCup: 0.9994\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.95      0.98        21\n","           1       1.00      0.00      0.00         1\n","           2       1.00      0.00      0.00         1\n","           3       1.00      0.96      0.98        84\n","           6       1.00      1.00      1.00      3795\n","           7       0.92      0.88      0.90        26\n","           8       1.00      1.00      1.00      6384\n","           9       1.00      0.00      0.00         1\n","          10       1.00      0.97      0.99        73\n","          11       1.00      0.99      1.00       135\n","          12       1.00      1.00      1.00     19466\n","          13       1.00      1.00      1.00         1\n","          14       1.00      0.50      0.67        12\n","\n","    accuracy                           1.00     30000\n","   macro avg       0.99      0.71      0.73     30000\n","weighted avg       1.00      1.00      1.00     30000\n","\n","\n","🔹 Training on CICIDS2017 dataset...\n","\n","✅ CICIDS2017 dataset limited to 100000 rows.\n","Class distribution in training set: {0: 70000}\n","🎯 Random Forest Accuracy for CICIDS2017: 1.0000\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     30000\n","\n","    accuracy                           1.00     30000\n","   macro avg       1.00      1.00      1.00     30000\n","weighted avg       1.00      1.00      1.00     30000\n","\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report\n","from google.colab import drive\n","\n","# ✅ Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# ✅ File paths\n","file_paths = [\n","    \"/content/drive/MyDrive/Datasets/NSLKDD.csv\",\n","    \"/content/drive/MyDrive/Datasets/UNSW_NB15_merged.csv\",\n","    \"/content/drive/MyDrive/Datasets/kddcup.csv\",\n","    \"/content/drive/MyDrive/Datasets/CICIDS2017.csv\"\n","]\n","dataset_names = [\"NSLKDD\", \"UNSW_NB15\", \"KDDCup\", \"CICIDS2017\"]\n","\n","# ✅ Correct target column names\n","target_columns = {\n","    \"NSLKDD\": \"anomaly\",\n","    \"UNSW_NB15\": \"label\",\n","    \"KDDCup\": \"Label\",\n","    \"CICIDS2017\": \" Label\"\n","}\n","\n","print(\"\\n==== Training Random Forest Models for Each Dataset ====\\n\")\n","\n","for file, name in zip(file_paths, dataset_names):\n","    print(f\"\\n🔹 Training on {name} dataset...\\n\")\n","\n","    try:\n","        df = pd.read_csv(file, low_memory=False).dropna(axis=1, how='all')\n","    except FileNotFoundError:\n","        print(f\"❌ Error: {file} not found. Skipping {name}.\")\n","        continue\n","\n","    # ✅ Limit dataset to 100,000 rows\n","    row_limit = 100000\n","    if len(df) > row_limit:\n","        df = df.sample(n=row_limit, random_state=42)\n","        print(f\"✅ {name} dataset limited to {row_limit} rows.\")\n","\n","    target_column = target_columns.get(name)\n","    if target_column not in df.columns:\n","        print(f\"⚠ Skipping {name}, target column '{target_column}' not found!\")\n","        continue\n","\n","    X = df.drop(columns=[target_column])\n","    y = df[target_column]\n","\n","    # ✅ Convert categorical columns to numerical\n","    for col in X.select_dtypes(include=['object']).columns:\n","        X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n","\n","    # ✅ Handle infinite and NaN values before scaling\n","    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    X.fillna(X.median(), inplace=True)\n","\n","    # ✅ Scale features\n","    X = StandardScaler().fit_transform(X)\n","    y = LabelEncoder().fit_transform(y)\n","\n","    # ✅ Train-Test Split (70-30)\n","    split = int(0.7 * len(X))\n","    X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]\n","\n","    # ✅ Check class distribution\n","    unique, counts = np.unique(y_train, return_counts=True)\n","    print(f\"Class distribution in training set: {dict(zip(unique, counts))}\")\n","\n","    # ✅ Train Random Forest Model\n","    clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n","    clf.fit(X_train, y_train)\n","\n","    # ✅ Predictions\n","    y_pred = clf.predict(X_test)\n","\n","    # ✅ Model Evaluation\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f\"🎯 Random Forest Accuracy for {name}: {accuracy:.4f}\")\n","    print(classification_report(y_test, y_pred, zero_division=1))\n"]}]}